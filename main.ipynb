{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "#nltk.data.path.append('C:\\\\Users\\\\della\\\\anaconda3\\\\nltk-data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pre_processing\n",
    "#importlib.reload(pre_processing)\n",
    "\n",
    "import text_vectorization\n",
    "#importlib.reload(text_vectorization)\n",
    "\n",
    "import embedding\n",
    "#importlib.reload(embedding)\n",
    "\n",
    "import kfold_cv\n",
    "#importlib.reload(kfold_cv)\n",
    "\n",
    "import data_augmentation\n",
    "importlib.reload(data_augmentation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Reading initial dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv('data/arxiv-dataset-cat1.csv')\n",
    "\n",
    "# Number of different categories\n",
    "number_of_categories = len(arxiv_data['label'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Splitting in training and test set*\n",
    "\n",
    "The initial dataset is partitioned in training and test set, with the following strategy:\n",
    "- **70%** training and **30%** test;\n",
    "- **Stratified sampling** based on the value of the label (categeory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(arxiv_data, test_size = 0.3, stratify = arxiv_data['label'], random_state = 19)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Pre-processing on training set*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A phase of pre-processing is applied to the textual observation of the training set, with the following operations:\n",
    "- Converting in **lower case**;\n",
    "- Removing **special characters and symbols**;\n",
    "- Removing **stop words**;\n",
    "- **Lemmatization**.\n",
    "\n",
    "Therefore, the labels associated with articles' categories are converted in **one-hot representation**.\n",
    "\n",
    "*The processed dataset has been saved in a CSV file, in such a way that it is not necessary to repeat the time-consuming procedure each time the code is executed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set_processed = pre_processing.dataPreProcessing(train_set)\n",
    "#train_set_processed.to_csv('data/train-set-cat1-processed.csv', index = False)\n",
    "\n",
    "train_set = pd.read_csv('data/train-set-cat1-processed.csv')\n",
    "\n",
    "# One-hot encoding of the labels\n",
    "label_train = to_categorical(train_set['label'], num_classes = number_of_categories, dtype = 'int64') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data augmentation*\n",
    "We have decided to use some **data augmentation techniques** with the aim of balancing the training set, considering these issues:\n",
    "- **Number of words per observation**\n",
    "\n",
    "    About this problem, a **threshold** of at least **15 words per observation** has been setted. In order to obtain this result, the **Random Insertion** technique has been used.\\\n",
    "    Our developed **algorithm** follows these steps:\n",
    "    \n",
    "    1. For each observation which does have less words than the threshold, get a **random sample of 5 words** from the initial text;\n",
    "    2. Then, get the **5 most similar words** for each of the previously retrieved tokens and from this set, get **n random words** (we set *n* equal to *15*);\n",
    "    3. Insert the **n words** in random positions within the starting text and substitute the augmented text in the train set;\n",
    "    4. Redo the **pre-processing phase**, due to the fact that the random insertion could have include words we do not accept in our context (like stop words). In this way, we also apply the formatting;\n",
    "    5. **Filter** another time the train set with the aim of finding \"under-threshold\" observations (pre-processing could have deleted some words);\n",
    "    6. Repeat the algorithm until there are no more observations to augment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_augmented = train_set.copy()\n",
    "\n",
    "# Count the number of words for each observation\n",
    "train_set_augmented['words'] = train_set_augmented['text'].apply(lambda row: len(row.split()))\n",
    "\n",
    "# Min number of words per observation\n",
    "min_number_words = 15\n",
    "\n",
    "# Set of observations with less than the threshold (needed data augmentation)\n",
    "observations_to_augment = train_set_augmented[train_set_augmented['words'] < min_number_words]\n",
    "\n",
    "# Only if there are still observations to augment\n",
    "while(len(observations_to_augment) != 0):\n",
    "\n",
    "    # Creating the augmented observations\n",
    "    obs_augmented = observations_to_augment['text'].apply(lambda row: data_augmentation.randomInsert(row, min_number_words))\n",
    "\n",
    "    # For each augmented observation replace the text in the training set and redo the pre-processing\n",
    "    for i in obs_augmented.index:\n",
    "\n",
    "        train_set_augmented.loc[i,'text'] = obs_augmented[i]\n",
    "        train_set_augmented.loc[i,'text'] = pre_processing.dataPreProcessing(pd.DataFrame(train_set_augmented.loc[i,:]).transpose()).loc[i,'text']\n",
    "        train_set_augmented.loc[i,'words'] = len(train_set_augmented.loc[i,'text'].split())\n",
    "\n",
    "    observations_to_augment = train_set_augmented[train_set_augmented['words'] < min_number_words]\n",
    "\n",
    "# Drop the unuseful column and write the CSV (previous operations are computational expensive)\n",
    "train_set_augmented.drop('words', axis = 1, inplace = True)\n",
    "train_set_augmented.to_csv('data/train-set-cat1-augmented-words-obs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_augmented = pd.read_csv('data/train-set-cat1-augmented-words-obs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Class imbalance management**\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.33\n",
       "0    0.26\n",
       "3    0.21\n",
       "5    0.06\n",
       "6    0.06\n",
       "7    0.04\n",
       "2    0.03\n",
       "1    0.02\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(train_set_augmented['label'].value_counts() / sum(train_set['label'].value_counts()), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Text vectorization and embedding*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstable the following **parameters** are defined:\n",
    "- **Size of the vocabulary** to create;\n",
    "- **Number of words** considered for each text (article);\n",
    "- **Dimension of the embedding**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "words_per_sentence = 200\n",
    "embedding_dim = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have opted for **two different approaches**:\n",
    "\n",
    "- **Version 1: Keras vectorization and GloVe embedding**\\\n",
    "In this scenario, the vectorization (and so the creation of the vocabulary) is carried out using the **Keras built-in function**, with the final adaption of the text vectorizer on the training set. For the embedding matrix, we have used a pre-trained solution, named **GloVe**, with 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer_keras = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n",
    "vocabulary_keras = text_vectorizer_keras.get_vocabulary()\n",
    "\n",
    "embedding_matrix_glove = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary_keras)\n",
    "embedding_layer_glove = embedding.createEmbeddingLayer(embedding_matrix_glove, None)\n",
    "\n",
    "feature_train_glove = text_vectorization.textVectorization(train_set['text'], text_vectorizer_keras)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Version 2: Word2Vec vectorization and embedding**\\\n",
    "The second strategy plans to use a text vectorizer, a vocabulary and an embedding using a **Word2Vec model** directly trained and created on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = text_vectorization.createTextVectorizerWord2Vec(train_set['text'], vocabulary_size, embedding_dim)\n",
    "text_vectorizer_word2vec = word2vec['text_vectorizer']\n",
    "vocabulary_word2vec = list(word2vec['vocabulary_embedding'].key_to_index)\n",
    "\n",
    "embedding_matrix_word2vec = embedding.buildingEmbeddingMatrixWord2Vec(embedding_dim, vocabulary_word2vec, word2vec['vocabulary_embedding'])\n",
    "embedding_layer_word2vec = embedding.createEmbeddingLayer(embedding_matrix_word2vec, None)\n",
    "\n",
    "feature_train_word2vec = text_vectorization.textVectorizationWord2Vec(train_set['text'], text_vectorizer_word2vec, words_per_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Neural network architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n",
    "\n",
    "# Embedding layer\n",
    "x = embedding_layer_glove(input_layer)\n",
    "\n",
    "# Hidden layers\n",
    "x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(rate = 0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n",
    "output_layer = x\n",
    "\n",
    "# Neural network model\n",
    "network = keras.Model(input_layer, output_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *K-Fold Cross Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'batch_size': 128\n",
    "\n",
    "}, \n",
    "\n",
    "{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': 128\n",
    "\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_results = kfold_cv.kfoldCrossValidation(2, feature_train, label_train, network, hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
