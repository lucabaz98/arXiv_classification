{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "#nltk.data.path.append('C:\\\\Users\\\\della\\\\anaconda3\\\\nltk-data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import text_vectorization\n",
    "#importlib.reload(text_vectorization)\n",
    "\n",
    "import embedding\n",
    "#importlib.reload(embedding)\n",
    "\n",
    "import kfold_cv\n",
    "#importlib.reload(kfold_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also computed the **number of unique categories**, that will be useful later during the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('data/train-set-cat1-processed.csv')\n",
    "train_set_augmented = pd.read_csv('data/train-set-cat1-augmented.csv')\n",
    "test_set = pd.read_csv('data/test-set-cat1-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of different categories\n",
    "number_of_categories = len(train_set['label'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Text vectorization and embedding*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the following **parameters** are defined:\n",
    "- **Size of the vocabulary** to create;\n",
    "- **Number of words** considered for each text (article);\n",
    "- **Dimension of the embedding**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "words_per_sentence = 200\n",
    "embedding_dim = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have opted for **two different approaches**:\n",
    "\n",
    "- **Version 1: Keras vectorization and GloVe embedding**\\\n",
    "In this scenario, the vectorization (and so the creation of the vocabulary) is carried out using the **Keras built-in function**, with the final adaption of the text vectorizer on the training set. For the embedding matrix, we have used a pre-trained solution, named **GloVe**, with 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer_keras = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n",
    "vocabulary_keras = text_vectorizer_keras.get_vocabulary()\n",
    "\n",
    "embedding_matrix_glove = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary_keras)\n",
    "embedding_layer_glove = embedding.createEmbeddingLayer(embedding_matrix_glove, None)\n",
    "\n",
    "feature_train_glove = text_vectorization.textVectorization(train_set['text'], text_vectorizer_keras)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Version 2: Word2Vec vectorization and embedding**\\\n",
    "The second strategy plans to use a text vectorizer, a vocabulary and an embedding using a **Word2Vec model** directly trained and created on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = text_vectorization.createTextVectorizerWord2Vec(train_set['text'], vocabulary_size, embedding_dim)\n",
    "text_vectorizer_word2vec = word2vec['text_vectorizer']\n",
    "vocabulary_word2vec = list(word2vec['vocabulary_embedding'].key_to_index)\n",
    "\n",
    "embedding_matrix_word2vec = embedding.buildingEmbeddingMatrixWord2Vec(embedding_dim, vocabulary_word2vec, word2vec['vocabulary_embedding'])\n",
    "embedding_layer_word2vec = embedding.createEmbeddingLayer(embedding_matrix_word2vec, None)\n",
    "\n",
    "feature_train_word2vec = text_vectorization.textVectorizationWord2Vec(train_set['text'], text_vectorizer_word2vec, words_per_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Neural network architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n",
    "\n",
    "# Embedding layer\n",
    "x = embedding_layer_glove(input_layer)\n",
    "\n",
    "# Hidden layers\n",
    "x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(rate = 0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n",
    "output_layer = x\n",
    "\n",
    "# Neural network model\n",
    "network = keras.Model(input_layer, output_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *K-Fold Cross Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'batch_size': 128\n",
    "\n",
    "}, \n",
    "\n",
    "{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': 128\n",
    "\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_results = kfold_cv.kfoldCrossValidation(2, feature_train, label_train, network, hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
