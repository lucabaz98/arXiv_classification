{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score, log_loss, roc_curve, roc_auc_score\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "Here, you can find the implementation of the K-Fold Cross Validation.\n",
    "\n",
    "The **parameters** of the function are:\n",
    "\n",
    "- *k_folds* : number of folds\n",
    "- *training_set* : training dataset on which perform the CV\n",
    "- *neural_network* : a Neural Network architecture\n",
    "- *hyper_params* : a list of objects with hyperparameters combinations to try (they need to be consistent with the NN architecture layers and properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldCrossValidation(k_folds, feature, label, neural_network, hyper_params):\n",
    "\n",
    "    # Stratified K-fold Cross Validation\n",
    "    stratified_kfold = StratifiedKFold(n_splits = k_folds, random_state = 19, shuffle = True)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for hyper_params_combination in hyper_params:\n",
    "    \n",
    "        print(hyper_params_combination)\n",
    "\n",
    "        # List with evaluation metric (performance for each iteration)\n",
    "        evaluation_metric = []\n",
    "    \n",
    "        # Neural Network architecture with hyperparameters combination\n",
    "        # ... #\n",
    "\n",
    "        # Splitting in training and validation set\n",
    "        for train, val in stratified_kfold.split(feature, label):\n",
    "\n",
    "            # Training (fit Neural Network)\n",
    "            neural_network.fit(\n",
    "                x = feature[train], y = label[train], \n",
    "                batch_size = hyper_params_combination['batch_size'], \n",
    "                epochs = hyper_params_combination['epoch'], \n",
    "                validation_data = (feature[val], label[val])\n",
    "            )\n",
    "            \n",
    "            # Validation \n",
    "            # ... #\n",
    "\n",
    "            \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which performs both final training and testing\n",
    "def trainingAndTesting(n_estimators, training_set):\n",
    "\n",
    "    # Feature (normalized in [0,1]) and label 'clicker' for both training and testing set\n",
    "    label_train = training_set['clicker'].to_numpy()\n",
    "    feature_train = MinMaxScaler().fit_transform(training_set.drop('clicker', axis = 1).to_numpy())\n",
    "    label_test = testing_set['clicker'].to_numpy()\n",
    "    feature_test = MinMaxScaler().fit_transform(testing_set.drop('clicker', axis = 1).to_numpy())\n",
    "\n",
    "    # Training\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        criterion = 'entropy', n_estimators = n_estimators, \n",
    "        class_weight = 'balanced_subsample', random_state = 19\n",
    "    )\n",
    "    rf_classifier.fit(feature_train, label_train)\n",
    "\n",
    "    # Testing\n",
    "    test_pred_prob = rf_classifier.predict_proba(feature_test)\n",
    "    test_pred_class = rf_classifier.predict(feature_test)\n",
    "\n",
    "    testing_data = pd.DataFrame({\n",
    "\n",
    "        'actual': label_test.tolist(), \n",
    "        'pred_class': test_pred_class,\n",
    "        'pred_prob_neg': test_pred_prob[:, 0],\n",
    "        'pred_prob_pos': test_pred_prob[:, 1] \n",
    "        \n",
    "    })  \n",
    "\n",
    "    # Dataframe with only positive and negative class observations\n",
    "    positive_class = testing_data[testing_data['actual'] == 1].copy()\n",
    "    negative_class = testing_data[testing_data['actual'] == 0].copy()\n",
    "\n",
    "    # Deciles of the predicted probabilities\n",
    "    deciles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    positive_class['deciles'] = pd.cut(positive_class['pred_prob_pos'], deciles, include_lowest = True, right = False)\n",
    "    negative_class['deciles'] = pd.cut(negative_class['pred_prob_neg'], deciles, include_lowest = True, right = False)\n",
    "\n",
    "    # Deciles data with frequency and ratio of observation of positive class in the deciles\n",
    "    deciles_data = pd.DataFrame({\n",
    "        'pos': positive_class['deciles'].value_counts(sort = False),\n",
    "        'neg': negative_class['deciles'].value_counts(sort = False)\n",
    "    })\n",
    "\n",
    "    deciles_data.reset_index(inplace = True)\n",
    "    deciles_data['ratio_pos'] = deciles_data.apply(lambda row: round(row['pos'] / len(positive_class), 3), axis = 1)\n",
    "    deciles_data['ratio_neg'] = deciles_data.apply(lambda row: round(row['neg'] / len(negative_class), 3), axis = 1)\n",
    "\n",
    "    deciles_data.columns = ['decile_interval', 'freq_pos', 'freq_neg', 'ratio_pos', 'ratio_neg']\n",
    "    \n",
    "    # Performance metrics computation \n",
    "    logloss = round(log_loss(y_true = testing_data['actual'], \n",
    "                             y_pred = np.column_stack((testing_data['pred_prob_neg'], testing_data['pred_prob_pos']))), 3)\n",
    "    \n",
    "    fp_ratio, tp_ratio, thresholds = roc_curve(testing_data['actual'], testing_data['pred_class'])\n",
    "    roc_auc = roc_auc_score(testing_data['actual'], testing_data['pred_class'])\n",
    "    \n",
    "    return { \n",
    "        'recall': round(recall_score(testing_data['actual'], testing_data['pred_class']), 3),\n",
    "        'f1_score': round(f1_score(testing_data['actual'], testing_data['pred_class']), 3),\n",
    "        'deciles_data': deciles_data, \n",
    "        'log_loss': logloss,\n",
    "        'ROC': [ fp_ratio, tp_ratio, thresholds, roc_auc ]\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
