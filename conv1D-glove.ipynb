{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network - Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# from shutil import copyfile\n",
    "# copyfile('/content/drive/MyDrive/FDL Project/Code/text_vectorization.py', 'text_vectorization.py')\n",
    "# copyfile('/content/drive/MyDrive/FDL Project/Code/embedding.py', 'embedding.py')\n",
    "# copyfile('/content/drive/MyDrive/FDL Project/Code/kfold_cv.py', 'kfold_cv.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kfold_cv' from 'c:\\\\Users\\\\della\\\\Desktop\\\\arXiv_classification\\\\kfold_cv.py'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from tensorflow import keras\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import text_vectorization\n",
    "#importlib.reload(text_vectorization)\n",
    "\n",
    "import embedding\n",
    "#importlib.reload(embedding)\n",
    "\n",
    "import kfold_cv\n",
    "importlib.reload(kfold_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first stage, we have:\n",
    "- Read the **training and test set**;\n",
    "- Calculated the **number of unique categories**, so the number of classes in the text classification;\n",
    "- Converted the labels associated with the articles' to **one-hot encoding representation**, which is a deep learning best practice when we cope with multi-label text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = pd.read_csv('/content/drive/MyDrive/FDL Project/Code/data/train-set-cat1-processed.csv')\n",
    "# test_set = pd.read_csv('/content/drive/MyDrive/FDL Project/Code/data/test-set-cat1-processed.csv')\n",
    "\n",
    "train_set = pd.read_csv('data/train-set-cat1-processed.csv')\n",
    "test_set = pd.read_csv('data/test-set-cat1-processed.csv')\n",
    "\n",
    "# Number of different categories\n",
    "number_of_categories = len(train_set['label'].unique())\n",
    "\n",
    "# One-hot encoding of the labels\n",
    "label_train = to_categorical(train_set['label'], num_classes = number_of_categories, dtype = 'int64')\n",
    "label_test = to_categorical(test_set['label'], num_classes = number_of_categories, dtype = 'int64') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Text vectorization and embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the following **parameters** are defined:\n",
    "- **Size of the vocabulary** to create;\n",
    "- **Number of words** considered for each text (article);\n",
    "- **Dimension of the embedding**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "words_per_sentence = 200\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have opted the first embedding approach: **Keras vectorization and GloVe embedding**.\n",
    "\n",
    "- The *vectorization* (and so the creation of the *vocabulary*) is carried out using the **Keras built-in function**, with the final adaption of the text vectorizer on the training set;\n",
    "- For the *embedding matrix*, we have used a pre-trained solution, named **GloVe**, with 100 dimensions;\n",
    "- Finally, we have created the final **vectorized feature** for the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer_keras = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n",
    "vocabulary_keras = text_vectorizer_keras.get_vocabulary()\n",
    "\n",
    "embedding_matrix_glove = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary_keras)\n",
    "embedding_layer_glove = embedding.createEmbeddingLayer(embedding_matrix_glove, None)\n",
    "embedding_layer_glove._name = 'GloVe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_glove = text_vectorization.textVectorization(train_set['text'], text_vectorizer_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Neural network architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n",
    "\n",
    "# Embedding layer\n",
    "x = embedding_layer_glove(input_layer)\n",
    "\n",
    "# Hidden layers\n",
    "x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(rate = 0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n",
    "output_layer = x\n",
    "\n",
    "# Neural network model\n",
    "network = keras.Model(input_layer, output_layer, name = 'Conv1D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [\n",
    "    \n",
    "    {\n",
    "\n",
    "        'filters': 128,\n",
    "        'kernel_size': 5,\n",
    "        'rate': 0.5,\n",
    "        'optimizer': 'rmsprop',\n",
    "        'batch_size': 128\n",
    "\n",
    "    }\n",
    "\n",
    "    #  ,\n",
    "    #   {\n",
    "    #  'filters': 128,\n",
    "    #  'kernel_size': 5,\n",
    "    #  'rate': 0.5,\n",
    "    #  'optimizer': 'adam',\n",
    "    #  'batch_size': 128\n",
    "    #  }\n",
    "\n",
    "]\n",
    "\n",
    "epochs = 1\n",
    "k_fold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 13s 58ms/step - loss: 1.2190 - accuracy: 0.6000\n",
      "212/212 [==============================] - 12s 55ms/step - loss: 0.9178 - accuracy: 0.6999\n"
     ]
    }
   ],
   "source": [
    "kfold_results = kfold_cv.kfoldCrossValidation(k_fold, feature_train_glove, label_train, network, hyperparams, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Network': 'Conv1D',\n",
       "  'Embedding': 'GloVe',\n",
       "  'k_folds': 2,\n",
       "  'filters': 128,\n",
       "  'kernel_size': 5,\n",
       "  'rate': 0.5,\n",
       "  'optimizer': 'rmsprop',\n",
       "  'batch_size': 128,\n",
       "  'loss_kfold': 0.789,\n",
       "  'accuracy_kfold': 0.742,\n",
       "  'best_number_epochs': 1,\n",
       "  'n_epochs': 1}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Network': 'Conv1D', 'Embedding': 'GloVe', 'k_folds': 2, 'filters': 128, 'kernel_size': 5, 'rate': 0.5, 'optimizer': 'rmsprop', 'batch_size': 128, 'loss_kfold': 0.789, 'accuracy_kfold': 0.742, 'best_number_epochs': 1, 'n_epochs': 1}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams = { 'loss_kfold': 999 }\n",
    "\n",
    "for result in kfold_results:\n",
    "\n",
    "    if(result['loss_kfold'] < best_hyperparams['loss_kfold']):\n",
    "        best_hyperparams = result\n",
    "\n",
    "print(best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network architecture with best hyperparameters combination\n",
    "network.layers[2].filters = best_hyperparams['filters']\n",
    "network.layers[2].kernel_size = (best_hyperparams['kernel_size'],)\n",
    "network.layers[4].rate = best_hyperparams['rate']\n",
    "\n",
    "# Compiling the network\n",
    "network.compile(\n",
    "\n",
    "    loss = 'categorical_crossentropy', \n",
    "    optimizer = best_hyperparams['optimizer'], \n",
    "    metrics = ['accuracy']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423/423 [==============================] - 23s 54ms/step - loss: 1.0561 - accuracy: 0.6545\n"
     ]
    }
   ],
   "source": [
    "# Training (fit Neural Network)\n",
    "training_history = network.fit(\n",
    "\n",
    "    x = feature_train_glove,\n",
    "    y = label_train,\n",
    "    batch_size = best_hyperparams['batch_size'],\n",
    "    epochs = best_hyperparams['best_number_epochs']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test_glove = text_vectorization.textVectorization(test_set['text'], text_vectorizer_keras)\n",
    "\n",
    "# Validation \n",
    "score = network.evaluate(feature_test_glove, label_test, verbose = 0)\n",
    "\n",
    "# Performance metrics\n",
    "test_loss = score[0]\n",
    "test_accuracy = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.747"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(test_accuracy, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
