{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "#nltk.data.path.append('C:\\\\Users\\\\della\\\\anaconda3\\\\nltk-data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pre_processing\n",
    "#importlib.reload(pre_processing)\n",
    "\n",
    "import text_vectorization\n",
    "#importlib.reload(text_vectorization)\n",
    "\n",
    "import embedding\n",
    "#importlib.reload(embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv('data/arxiv-dataset-cat1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_categories = len(arxiv_data['label'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(arxiv_data, test_size = 0.3, stratify = arxiv_data['label'], random_state = 19)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set_processed = pre_processing.dataPreProcessing(train_set)\n",
    "#train_set_processed.to_csv('data/train-set-cat1-processed.csv', index = False)\n",
    "train_set = pd.read_csv('data/train-set-cat1-processed.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "words_per_sentence = 200\n",
    "embedding_dim = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n",
    "vocabulary = text_vectorizer.get_vocabulary()\n",
    "\n",
    "embedding_matrix = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary)\n",
    "embedding_layer = embedding.createEmbeddingLayer(embedding_matrix, None)\n",
    "\n",
    "feature_train = text_vectorization.textVectorization(train_set['text'], text_vectorizer)\n",
    "label_train = to_categorical(train_set['label'], num_classes = number_of_categories, dtype = 'int64') # One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n",
    "\n",
    "# Embedding layer\n",
    "x = embedding_layer(input_layer)\n",
    "\n",
    "# Hidden layers\n",
    "x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(rate = 0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n",
    "output_layer = x\n",
    "\n",
    "# Neural network model\n",
    "network = keras.Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import tensorflow\n",
    "\n",
    "def kfoldCrossValidation(k_folds, feature, label, neural_network, hyperparams):\n",
    "\n",
    "    # Stratified K-fold Cross Validation\n",
    "    stratified_kfold = MultilabelStratifiedKFold(n_splits = k_folds, random_state = 19, shuffle = True)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for hyperparams_combination in hyperparams:\n",
    "    \n",
    "        print(hyperparams_combination)\n",
    "\n",
    "        # List with evaluation metric (performance for each iteration)\n",
    "        evaluation_metric = []\n",
    "    \n",
    "        # Neural Network architecture with hyperparameters combination\n",
    "        network.layers[2].filters = hyperparams_combination['filters']\n",
    "        network.layers[2].kernel_size = hyperparams_combination['kernel_size']\n",
    "        network.layers[4].rate = hyperparams_combination['rate']\n",
    "\n",
    "        # Compiling the network\n",
    "        network.compile(\n",
    "            loss = 'categorical_crossentropy', \n",
    "            optimizer = hyperparams_combination['optimizer'], \n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "\n",
    "        # Converting to numpy for splitting\n",
    "        feature = feature.numpy()\n",
    "\n",
    "        # Splitting in training and validation set\n",
    "        for train, val in stratified_kfold.split(feature, label):\n",
    "\n",
    "            feature_train = tensorflow.convert_to_tensor(feature[train])\n",
    "            feature_val = tensorflow.convert_to_tensor(feature[val])\n",
    "\n",
    "            print(label[train])\n",
    "\n",
    "            # Training (fit Neural Network)\n",
    "            training_history = neural_network.fit(\n",
    "                x = feature_train, \n",
    "                y = label[train], \n",
    "                batch_size = hyperparams_combination['batch_size'], \n",
    "                epochs = hyperparams_combination['epochs']\n",
    "            )\n",
    "            \n",
    "            # Validation \n",
    "            score = neural_network.evaluate(feature_val, label[val], verbose = 0)\n",
    "\n",
    "            print(score)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'batch_size': 128,\n",
    "    'epochs': 15\n",
    "\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filters': 128, 'kernel_size': 5, 'rate': 0.5, 'optimizer': 'rmsprop', 'batch_size': 128, 'epochs': 15}\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\della\\anaconda3\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\della\\anaconda3\\keras\\layers\\convolutional\\base_conv.py\", line 328, in <listcomp>\n        self.kernel_size[i],\n\n    TypeError: Exception encountered when calling layer 'conv1d_6' (type Conv1D).\n    \n    'int' object is not subscriptable\n    \n    Call arguments received by layer 'conv1d_6' (type Conv1D):\n      • inputs=tf.Tensor(shape=(None, 200, 100), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m kfoldCrossValidation(\u001b[39m3\u001b[39;49m, feature_train, label_train, network, hyperparams)\n",
      "Cell \u001b[1;32mIn[146], line 42\u001b[0m, in \u001b[0;36mkfoldCrossValidation\u001b[1;34m(k_folds, feature, label, neural_network, hyperparams)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(label[train])\n\u001b[0;32m     41\u001b[0m \u001b[39m# Training (fit Neural Network)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m training_history \u001b[39m=\u001b[39m neural_network\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     43\u001b[0m     x \u001b[39m=\u001b[39;49m feature_train, \n\u001b[0;32m     44\u001b[0m     y \u001b[39m=\u001b[39;49m label[train], \n\u001b[0;32m     45\u001b[0m     batch_size \u001b[39m=\u001b[39;49m hyperparams_combination[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m     46\u001b[0m     epochs \u001b[39m=\u001b[39;49m hyperparams_combination[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[39m# Validation \u001b[39;00m\n\u001b[0;32m     50\u001b[0m score \u001b[39m=\u001b[39m neural_network\u001b[39m.\u001b[39mevaluate(feature_val, label[val], verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\della\\anaconda3\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexzawtpiz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\della\\anaconda3\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\della\\anaconda3\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\della\\anaconda3\\keras\\layers\\convolutional\\base_conv.py\", line 328, in <listcomp>\n        self.kernel_size[i],\n\n    TypeError: Exception encountered when calling layer 'conv1d_6' (type Conv1D).\n    \n    'int' object is not subscriptable\n    \n    Call arguments received by layer 'conv1d_6' (type Conv1D):\n      • inputs=tf.Tensor(shape=(None, 200, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "kfoldCrossValidation(3, feature_train, label_train, network, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verion 2: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = text_vectorization.createTextVectorizerWord2Vec(train_set['text'], vocabulary_size, embedding_dim)\n",
    "text_vectorizer = word2vec['text_vectorizer']\n",
    "vocabulary = list(word2vec['vocabulary_embedding'].key_to_index)\n",
    "\n",
    "embedding_matrix = embedding.buildingEmbeddingMatrixWord2Vec(embedding_dim, vocabulary, word2vec['vocabulary_embedding'])\n",
    "embedding_layer = embedding.createEmbeddingLayer(embedding_matrix, None)\n",
    "\n",
    "feature_train = text_vectorization.textVectorizationWord2Vec(train_set['text'], text_vectorizer, words_per_sentence)\n",
    "label_train = to_categorical(train_set['label'], num_classes = number_of_categories) # One-hot encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
