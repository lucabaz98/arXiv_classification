{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:44.605079700Z",
     "start_time": "2023-06-08T08:19:44.448838200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import json\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "data_folder = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.614653600Z",
     "start_time": "2023-06-08T08:19:44.464460700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_folder + \"/arxiv-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.661454800Z",
     "start_time": "2023-06-08T08:19:46.614653600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for abstract in df[\"abstract\"]:\n",
    "    inputs.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.692701700Z",
     "start_time": "2023-06-08T08:19:46.661454800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names = sorted(set(df[\"categoryGroup\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.849012500Z",
     "start_time": "2023-06-08T08:19:46.692701700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "labels = []\n",
    "for name in class_names:\n",
    "    for item in df[\"categoryGroup\"]:\n",
    "        if item == name:\n",
    "            labels.append(index)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.942684200Z",
     "start_time": "2023-06-08T08:19:46.849012500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77208"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of available messages\n",
    "data_card = len(inputs)\n",
    "data_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.958308100Z",
     "start_time": "2023-06-08T08:19:46.880260400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply (the same) random shuffle to inputs and targets\n",
    "rng = np.random.default_rng()\n",
    "shuffler = rng.permutation(data_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:46.989557Z",
     "start_time": "2023-06-08T08:19:46.911434700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [inputs[i] for i in shuffler]\n",
    "labels = [labels[i] for i in shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.020802700Z",
     "start_time": "2023-06-08T08:19:46.973965800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61766"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the ratio of training-validation data and compute the corresponding number of elements\n",
    "split_fraction = 0.8\n",
    "N_train = int(split_fraction * data_card)\n",
    "N_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.020802700Z",
     "start_time": "2023-06-08T08:19:47.005179Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = inputs[0:N_train]\n",
    "y_train = labels[0:N_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.098938900Z",
     "start_time": "2023-06-08T08:19:47.020802700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_val = inputs[N_train:]\n",
    "y_val = labels[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.114555200Z",
     "start_time": "2023-06-08T08:19:47.036444300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15442"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.114555200Z",
     "start_time": "2023-06-08T08:19:47.052049600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert targets to pure arrays (we will take care of x_train and x_val later)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.114555200Z",
     "start_time": "2023-06-08T08:19:47.098938900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = 20000\n",
    "words_per_sentence = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:47.208289800Z",
     "start_time": "2023-06-08T08:19:47.098938900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer\n",
    "vectorizer = TextVectorization(max_tokens=total_words, output_sequence_length=words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:53.729319600Z",
     "start_time": "2023-06-08T08:19:47.161433Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the vocabulary in the vectorizer\n",
    "vectorizer.adapt(x_train)\n",
    "# Alternative for very large datasets: feed the corpus by batch\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(x_train).batch(128)\n",
    "# vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:53.807387600Z",
     "start_time": "2023-06-08T08:19:53.729319600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'of', 'and']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect vocabulary (e.g. the first 4 words)\n",
    "vectorizer.get_vocabulary()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:19:53.934703500Z",
     "start_time": "2023-06-08T08:19:53.807387600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       "array([[   2, 7925, 6027,   13,    2,    1,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test vectorizer (word-to-index vocabulary)\n",
    "vectorizer([[\"the cat sat on the sofa\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:27:49.169834300Z",
     "start_time": "2023-06-02T15:27:46.212258700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply vectorizer to all our data\n",
    "x_train = vectorizer(x_train).numpy()\n",
    "x_val = vectorizer(x_val).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "glove.6B.100d.txt troppo grande, si trova su GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:30:49.613829400Z",
     "start_time": "2023-06-02T15:30:29.321689600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(data_folder + '/glove.6B.100d.txt',  encoding=\"utf8\") as f:\n",
    "  # For each text line\n",
    "  for line in f:\n",
    "    # Separate the word-string from the 100-dimensional-vector-string\n",
    "    word, coeffs = line.split(maxsplit=1)\n",
    "    # Convert 100-dimensional vector string into a proper floating point vector\n",
    "    coeffs = np.fromstring(coeffs, \"f\", sep=\" \")\n",
    "    # Create a new dictionary entry\n",
    "    embeddings_index[word] = coeffs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:49:16.363440400Z",
     "start_time": "2023-06-02T15:49:16.332194300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:49:23.270982700Z",
     "start_time": "2023-06-02T15:49:23.123514800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Size of the matrix: 20002 rows x 100 dimensions\n",
    "#   20000 words from our vocabulary + word separator + [unknown word]\n",
    "#   100-dimensional representation from GloVe\n",
    "embedding_matrix = np.zeros((total_words+2, embedding_dim))\n",
    "\n",
    "# For each word in our vocabulary\n",
    "for i, word in enumerate(vectorizer.get_vocabulary()):\n",
    "    # Search corresponding embedding in GloVe,\n",
    "    # and add it in the correct row of the embedding matrix\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    # else: words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:49:31.994067300Z",
     "start_time": "2023-06-02T15:49:31.978420300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transfer the embedding matrix to a Keras Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    total_words+2,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Input layer (200 input words/tokens)\n",
    "* Embedding layer mapping each word(token) to a 100-dimensional vector\n",
    "* 1-dimensional convolution (mapping 128, filter size 5): perchè ciascun input è una riga di dati\n",
    "* global max-pooling: andrà ad eliminare parte delle informazioni\n",
    "* Dropout with 50% probability\n",
    "* Dense layer mapping to the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:49:41.425828700Z",
     "start_time": "2023-06-02T15:49:41.262545300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define architecture\n",
    "inputs = keras.Input(shape=(words_per_sentence,), dtype='int64')\n",
    "x = embedding_layer(inputs)\n",
    "x = keras.layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(20, activation='softmax')(x)\n",
    "outputs = x\n",
    "net = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are using 1-dimensional layers (Convolutional and Pooling) because each input is represented by a 1-dimensional vector.\n",
    "The Dropout is used to fight overfitting. During training 50% of the inputs are dropped (set to 0), in order to create noise in the inputs to the subsequent layers. During test, it does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:49:48.206665900Z",
     "start_time": "2023-06-02T15:49:48.144156600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          2000200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 196, 128)          64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,066,908\n",
      "Trainable params: 66,708\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualize the defined architecture\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:49:58.546039400Z",
     "start_time": "2023-06-02T15:49:58.522846900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile neural model\n",
    "net.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "history = net.fit(x_train, y_train, batch_size=128, epochs=15, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "salvo il modello allenato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "net.save(data_folder+\"firstModelTrained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "salvo file history.json per grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(history.history, open(data_folder+\"/firstModelTrainedHistory.json\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "leggo il file history.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:47:42.595637500Z",
     "start_time": "2023-06-02T16:47:42.564389500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = json.load(open(data_folder+\"/firstModelTrainedHistory.json\", 'r'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "carico il modello allenato\\\n",
    "(si trova su GDrive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:20:54.542850600Z",
     "start_time": "2023-06-08T08:20:52.053276900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(data_folder+'/FirstModelTrained.h5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visualize accuracy plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T16:48:17.036616100Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22b83006f20>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(history['acc'])\n",
    "plt.plot(history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inference usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:20:36.657752400Z",
     "start_time": "2023-06-08T08:20:36.610865800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  # Input as string\n",
    "input_sentence = 'The article reveals the main theoretical approaches to the analysis and study of the phenomenon of corruption. Special attention is paid to the consideration of the index approach to the analysis of corruption.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:20:38.793061100Z",
     "start_time": "2023-06-08T08:20:38.708328Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([   2,  163, 1898,    2,  204,  223,  292,    7,    2,   59,    4,\n",
       "         48,    3,    2,  945,    3, 5127,  363,  810,    9, 3296,    7,\n",
       "          2, 1985,    3,    2,  520,   58,    7,    2,   59,    3, 5127,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input as vocabulary indices\n",
    "input_vector = vectorizer(input_sentence)\n",
    "input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:20:57.994546300Z",
     "start_time": "2023-06-08T08:20:57.672374Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 217ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22900328, 0.03445108, 0.0217946 , 0.08389115, 0.43906447,\n",
       "        0.09128293, 0.04134462, 0.05916787]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedded matrix and output probability distribution\n",
    "output_probs = loaded_model.predict(input_vector[None, ...])\n",
    "output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:21:07.356765200Z",
     "start_time": "2023-06-08T08:21:07.278645200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T08:21:09.482037800Z",
     "start_time": "2023-06-08T08:21:09.435144900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Physics'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selection of highest probabilty class\n",
    "class_names[np.argmax(output_probs[0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
