{"cells":[{"cell_type":"markdown","metadata":{"id":"AyzoBaM-dYDW"},"source":["### ***Long Short-Time Memory - No Data Augmentation***"]},{"cell_type":"markdown","metadata":{"id":"fa9S0xVm4HWS"},"source":["#### ***Initial operations***"]},{"cell_type":"markdown","metadata":{"id":"ARX9Y6X5_Jtt"},"source":["Firstly, we have done some initial and setting operations, like connecting to our Google Drive folder and importing libraries and files useful for the project."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14209,"status":"ok","timestamp":1689883010262,"user":{"displayName":"Luca Bazzetto","userId":"13807913188244400445"},"user_tz":-120},"id":"OuxbTGxHdqfV","outputId":"e0885d6a-4b7c-47da-8ba9-120d33d0a051"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import importlib\n","import itertools\n","import csv\n","import sys\n","import os\n","\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from keras.regularizers import l2\n","\n","parent_folder = os.path.abspath('../../')\n","sys.path.append(parent_folder)\n","\n","from src.utils import text_vectorization\n","# importlib.reload(text_vectorization)\n","\n","from src.utils import embedding\n","# importlib.reload(embedding)\n","\n","from src.utils import kfold_cv\n","# importlib.reload(kfold_cv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZruVorigK9e"},"outputs":[],"source":["network_name = 'lstm-noaug'\n","model_name = 'lstm'"]},{"cell_type":"markdown","metadata":{"id":"IKq2O2PGdYDe"},"source":["#### ***Training and test set***"]},{"cell_type":"markdown","metadata":{"id":"uI1QZHeGdYDf"},"source":["In this first stage, we have:\n","- Read the **training and test set**;\n","- Calculated the **number of unique categories**, so the number of classes in the text classification;\n","- Converted the labels associated with the articles' to **one-hot encoding representation**, which is a deep learning best practice when we cope with multi-label text classification task."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N67R0w5Sv7ck"},"outputs":[],"source":["train_set = pd.read_csv('../../data/processed/train-set-cat1-processed.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YzTL-jJdYDf"},"outputs":[],"source":["test_set = pd.read_csv('../../data/processed/test-set-cat1-processed.csv')\n","\n","# Number of different categories\n","number_of_categories = len(train_set['label'].unique())\n","\n","# One-hot encoding of the labels\n","label_train = to_categorical(train_set['label'], num_classes = number_of_categories, dtype = 'int64')\n","label_test = to_categorical(test_set['label'], num_classes = number_of_categories, dtype = 'int64')"]},{"cell_type":"markdown","metadata":{"id":"LyGUN5BYdYDg"},"source":["#### ***Text vectorization and embedding***"]},{"cell_type":"markdown","metadata":{"id":"NubDDxHFdYDi"},"source":["Firstly, the following **parameters** are defined:\n","- **Size of the vocabulary** to create;\n","- **Number of words** considered for each text (article);\n","- **Dimension of the embedding**;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-mnTyoldYDj"},"outputs":[],"source":["vocabulary_size = 50000\n","words_per_sentence = 200\n","embedding_dim = 100"]},{"cell_type":"markdown","metadata":{"id":"9ivQsA3mdYDj"},"source":["Then, we have carried out two embedding approaches:\n","\n","- **Keras vectorization and GloVe embedding**\n","\n","  - The *vectorization* (and so the creation of the *vocabulary*) is carried out using the **Keras built-in function**, with the final adaption of the text vectorizer on the training set;\n","  - For the *embedding matrix*, we have used a pre-trained solution, named **GloVe**, with 100 dimensions;\n","  - Finally, we have created the final **vectorized feature** for the training phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTgwgvDudYDk"},"outputs":[],"source":["text_vectorizer_keras = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n","vocabulary_keras = text_vectorizer_keras.get_vocabulary()\n","\n","embedding_matrix_glove = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary_keras)\n","embedding_layer_glove = embedding.createEmbeddingLayer(embedding_matrix_glove, None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6cMmMiLw_rT"},"outputs":[],"source":["feature_train_glove = text_vectorization.textVectorization(train_set['text'], text_vectorizer_keras)"]},{"cell_type":"markdown","metadata":{"id":"ttng3QL_wxxi"},"source":["- **Word2Vec vectorization and embedding**\n","\n","  - This strategy plans to create a text vectorizer, a vocabulary and an embedding using a *Word2Vec model* directly trained on our training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioMHcdj3wyJR"},"outputs":[],"source":["word2vec = text_vectorization.createTextVectorizerWord2Vec(train_set['text'], vocabulary_size, embedding_dim)\n","text_vectorizer_word2vec = word2vec['text_vectorizer']\n","vocabulary_word2vec = list(word2vec['vocabulary_embedding'].key_to_index)\n","\n","embedding_matrix_word2vec = embedding.buildingEmbeddingMatrixWord2Vec(embedding_dim, vocabulary_word2vec, word2vec['vocabulary_embedding'])\n","embedding_layer_word2vec = embedding.createEmbeddingLayer(embedding_matrix_word2vec, None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cx4I5utMxAlF"},"outputs":[],"source":["feature_train_word2vec = text_vectorization.textVectorizationWord2Vec(train_set['text'], text_vectorizer_word2vec, words_per_sentence)"]},{"cell_type":"markdown","metadata":{"id":"X1aJfLrqdYDk"},"source":["#### ***Neural network architectures***"]},{"cell_type":"markdown","metadata":{"id":"mnHYHXk7hLhT"},"source":["Here, we have defined a **set of LSTM Neural Network architectures** (models), using different combinations of hyperparameters:\n","\n","- *Embedding layer*: Glove or Word2Vec (as explained before);\n","- *Number of LSTM hidden layers*: 1, 2 and 3;\n","- *Number of units in the LSTM layers*: 32, 64 or 128;\n","- *Dropout rate in the LSTM layers*;\n","- *Regularization*: L2 regularization in the convolutional layers.\n","\n","The following parameters have kept the same value in each architectures:\n","- *Learning rate*: 0.5;\n","- *Batch size*: 512."]},{"cell_type":"markdown","metadata":{"id":"xgGCT7qu9uRo"},"source":["There will be a **list of neural network architecture**, which also a brief explanation:"]},{"cell_type":"markdown","metadata":{"id":"ekjg8yF496YU"},"source":["##### **Neural network A - 3 Layers, Same Number of Units, Without Dropout**\n","  - *Glove embedding*;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QzcvG6y95i3"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 128, dropout = 0, return_sequences = True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0, return_sequences = True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_A = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_A_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_units': 128,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': 128,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': 128,\n","    'layer3_dropout': None\n","\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"B8wLxDdhAxu-"},"source":["##### **Neural network B - 3 Layers, Glove versus Word2Vec**\n","  - *Word2Vec embedding*;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FebPFxC7xa-3"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.LSTM(units = 128, dropout = 0, return_sequences = True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0, return_sequences = True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_B = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_B_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_units': 128,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': 128,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': 128,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"SNU_QwjTEKTn"},"source":["##### **Neural network C - 3 Layers, Dropout and Recurrent Dropout**\n","  - *Glove embedding*;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrlJJN_HEB3e"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 128, dropout = 0.2, return_sequences = True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.2, return_sequences = True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.2)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_C = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_C_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_units': 128,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': 128,\n","    'layer2_dropout': 0.2,\n","\n","    'layer3_units': 128,\n","    'layer3_dropout': 0.2\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"p9A-IslEECNv"},"source":["##### **Neural network D - 3 Layers, Increase Number of Units**\n","  - *Glove embedding*;\n","  - *1 LSTM layer*: number of units equal to 32, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxZ4ykOSELN7"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 32, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 64, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 128)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_D = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_D_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_units': 32,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': 64,\n","    'layer2_dropout': 0.2,\n","\n","    'layer3_units': 128,\n","    'layer3_dropout': 0.2\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"Tvfe4Hv5E5-7"},"source":["##### **Neural network E - 3 Layers, Increase Dropout and Recurrent Dropout**\n","  - *Glove embedding*;\n","  - *1 LSTM layer*: number of units equal to 32, dropout 0.4;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.4;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.4;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIeREkv0FF9Q"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 32, dropout = 0.4, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 64, dropout = 0.4, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.4)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_E = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_E_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_units': 32,\n","    'layer1_dropout': 0.4,\n","\n","    'layer2_units': 64,\n","    'layer2_dropout': 0.4,\n","\n","    'layer3_units': 128,\n","    'layer3_dropout': 0.4,\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"GVzaOudFFSxQ"},"source":["##### **Neural network F - 3 Layers, With Regularization**\n","  - *Glove embedding*;\n","  - *1 LSTM layer*: number of units equal to 32, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *With regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wau28AsrEk9W"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 32, dropout = 0.2, return_sequences=True, kernel_regularizer = l2(0.01))(x)\n","x = keras.layers.LSTM(units = 64, dropout = 0.2, return_sequences=True, kernel_regularizer = l2(0.01))(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.2, kernel_regularizer = l2(0.01))(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_F = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_F_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': True,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_units': 32,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': 64,\n","    'layer2_dropout': 0.2,\n","\n","    'layer3_units': 128,\n","    'layer3_dropout': 0.2\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"4eSGIKbgGNeE"},"source":["##### **Neural network G - 2 Layers, Same Number of Units, Without Dropout**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dl1OINsbGOB_"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 64, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_G = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_G_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': 64,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"FsI3vC4zGOXL"},"source":["##### **Neural network H - 2 Layers, Glove versus Word2Vec**\n","  - *Word2Vec*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0o_W8dP9GOs6"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 64, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_H = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_H_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': 64,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"Hqa7iT8_GPXL"},"source":["##### **Neural network I - 2 Layers, Dropout and Recurrent Dropout**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BRkLj7TGPst"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0.2, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 64, dropout = 0.2,)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_I = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_I_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': 64,\n","    'layer2_dropout': 0.2,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"9Fk3UwSWHqp-"},"source":["##### **Neural network L - 2 Layers, Increase Number of Units**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyzDB_2zHq8J"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0.2,return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.2)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_L = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_L_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': 128,\n","    'layer2_dropout': 0.2,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"-jHdbEhrH2vs"},"source":["##### **Neural network M - 2 Layers, Increase Dropout and Recurrent Dropout**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.4;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.4;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk3D8uB0H3AI"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0.4, return_sequences=True)(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.4)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_M = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_M_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': 0.4,\n","\n","    'layer2_units': 128,\n","    'layer2_dropout': 0.4,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"5wW7v20nIDA7"},"source":["##### **Neural network N - 2 Layers, With Regularization**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0.2;\n","  - *With regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mvM7y1WIDUh"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0.2, return_sequences=True, kernel_regularizer = l2(0.01))(x)\n","x = keras.layers.LSTM(units = 128, dropout = 0.2, kernel_regularizer = l2(0.01))(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_N = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_N_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': True,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': 128,\n","    'layer2_dropout': 0.2,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"STsvwv1BIuEQ"},"source":["##### **Neural network O - 1 Layer, Glove**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 128, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8mJRJIlIuZe"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 128, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_O = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_O_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_units': 128,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': None,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"eBaK3W2gJGAu"},"source":["##### **Neural network P - 1 Layer, Word2Vec**\n","  - *Word2Vec*;\n","  - *1 LSTM layer*: number of filters equal to 128, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7IS5FM9JFb1"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.LSTM(units = 128, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_P = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_P_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_units': 128,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': None,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"RhCmWgKdJK-a"},"source":["##### **Neural network Q - 1 Layer, Decrease Number of Units**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIhvO5nQJLPc"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_Q = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_Q_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': None,\n","\n","    'layer2_units': None,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"6578nzKYJmdc"},"source":["##### **Neural network R - 1 Layer, Dropout and Recurrent Dropout**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OspSpvaFJm5L"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0.2)(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_R = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_R_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': None,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"ZOVf6J7fJnM5"},"source":["##### **Neural network S - 1 Layer, With Regularization**\n","  - *Glove*;\n","  - *1 LSTM layer*: number of units equal to 64, dropout 0.2;\n","  - *With regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPZ_RMKrJnez"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.LSTM(units = 64, dropout = 0.2, kernel_regularizer = l2(0.01))(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","lstm_network_S = keras.Model(input_layer, output_layer, name = model_name)\n","\n","lstm_network_S_info = {\n","\n","    'network': model_name,\n","    'data_aug': False,\n","\n","    'embedding': 'Glove',\n","    'regularization': True,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_units': 64,\n","    'layer1_dropout': 0.2,\n","\n","    'layer2_units': None,\n","    'layer2_dropout': None,\n","\n","    'layer3_units': None,\n","    'layer3_dropout': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"s62OXiC5mRdV"},"source":["##### **Neural networks sets and other information**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Zd8MiSHdYDp"},"outputs":[],"source":["network_models_set = [\n","    lstm_network_A, lstm_network_B, lstm_network_C, lstm_network_D,\n","    lstm_network_E, lstm_network_F, lstm_network_G, lstm_network_H,\n","    lstm_network_I, lstm_network_L, lstm_network_M, lstm_network_N,\n","    lstm_network_O, lstm_network_P, lstm_network_Q, lstm_network_R,\n","    lstm_network_S\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecfgbYuPmQ00"},"outputs":[],"source":["initial_weights = [\n","    lstm_network_A.get_weights(), lstm_network_B.get_weights(), lstm_network_C.get_weights(), lstm_network_D.get_weights(),\n","    lstm_network_E.get_weights(), lstm_network_F.get_weights(), lstm_network_G.get_weights(), lstm_network_H.get_weights(),\n","    lstm_network_I.get_weights(), lstm_network_L.get_weights(), lstm_network_M.get_weights(), lstm_network_N.get_weights(),\n","    lstm_network_O.get_weights(), lstm_network_P.get_weights(), lstm_network_Q.get_weights(), lstm_network_R.get_weights(),\n","    lstm_network_S.get_weights()\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_SAbxsxZDLH"},"outputs":[],"source":["network_info_set = [\n","    lstm_network_A_info, lstm_network_B_info, lstm_network_C_info, lstm_network_D_info,\n","    lstm_network_E_info, lstm_network_F_info, lstm_network_G_info, lstm_network_H_info,\n","    lstm_network_I_info, lstm_network_L_info, lstm_network_M_info, lstm_network_N_info,\n","    lstm_network_O_info, lstm_network_P_info, lstm_network_Q_info, lstm_network_R_info,\n","    lstm_network_S_info\n","]"]},{"cell_type":"markdown","metadata":{"id":"ZRUDu3bzdYDn"},"source":["#### ***K-Fold Cross Validation***"]},{"cell_type":"markdown","metadata":{"id":"LZXGj84uhufb"},"source":["In this part of the project, we have implemented the **K-Fold Cross Validation** as a strategy to find the **best hyperparameters** for the neural network and also to have a **performance estimation** of the model on new and unseen data. Our approach has followed these logic:\n","\n","*   Firstly, we defined a **number of epochs** equal to *30*, which will be an upper bound in the actual number of epochs used to train the model, due to the fact that we have used an *early stopping monitoring rule*: if the performance does not improve for 3 straight epochs, the K-Fold cycle end and we keep the epoch number with the best performance as hypeparameter;\n","\n","*   The **number of folds K** has been set to *3* and a multi-label stratified approach has been carried out;\n","\n","*   As a text classification task (categorical label), the **loss function** has been the **categorical cross entropy**, which will result in a loss value. Our goal is to **minimize** this metric, in order to improve the performance of the model, so we have used it as our performance proxy. Also, we have taken into account the **accuracy**;\n","\n","*   To evaluate a single moodel (combination of hyperparameters), we have computed the **average of the performance** of the K iteration;\n","\n","*   *The best network architecture is the one which lead to the lowest loss*;\n","\n","*   *We write a CSV file with all the different networks architecture and the related obtained performance in the K-Fold CV*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoMaZxYegS6G"},"outputs":[],"source":["epochs = 30\n","k_fold = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYt207UhdYDq"},"outputs":[],"source":["kfold_results = []\n","\n","# For each neural network architecture\n","for index, element in enumerate(network_models_set):\n","\n","    # Print information to manage the situation during the process\n","    print(f\"Neural network {index}\")\n","\n","    # Performing the K-Fold Cross Validation\n","    if(network_info_set[index]['embedding'] == 'Glove'):\n","\n","      kfold_result = kfold_cv.kfoldCrossValidation(k_fold, feature_train_glove, label_train, element, network_info_set[index], epochs)\n","\n","    if(network_info_set[index]['embedding'] == 'Word2Vec'):\n","\n","      kfold_result = kfold_cv.kfoldCrossValidation(k_fold, feature_train_word2vec, label_train, element, network_info_set[index], epochs)\n","\n","    kfold_results.append(kfold_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHeeRZVxdYDr"},"outputs":[],"source":["# Write the K-Fold CV results to a CSV file\n","with open('../../results/data/kfold-' + network_name + '.csv', mode = 'w', newline = '') as file:\n","\n","    writer = csv.DictWriter(file, fieldnames = list(kfold_results[0].keys()))\n","    writer.writeheader()\n","\n","    for row_data in kfold_results:\n","        writer.writerow(row_data)"]},{"cell_type":"markdown","metadata":{"id":"eL-oU7khscLL"},"source":["#### ***LSTM Neural Network - Final Architecture***"]},{"cell_type":"markdown","metadata":{"id":"SVkGQIL1swA7"},"source":["Here, we have created the **neural network architecture** model with the best hyperparameters found in the K-Fold Cross Validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laknVnOQdYDr"},"outputs":[],"source":["# Get the best hyperparameters combination\n","best_loss = 999\n","best_network = None\n","best_network_info = None\n","\n","for index, element in enumerate(kfold_results):\n","\n","    if(element['loss'] < best_loss):\n","\n","        best_loss = element['loss']\n","\n","        best_network = network_models_set[index]\n","        best_network.set_weights(initial_weights[index])\n","\n","        best_network_info = network_info_set[index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73Zvs_DIdYDr"},"outputs":[],"source":["# Compiling the network\n","best_network.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"09aHqRE1dYDs"},"source":["#### ***Training***"]},{"cell_type":"markdown","metadata":{"id":"A1-LV85MuAvo"},"source":["Training the neural network model with all the training data and save the H5 model file."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242490,"status":"ok","timestamp":1689883460686,"user":{"displayName":"Luca Bazzetto","userId":"13807913188244400445"},"user_tz":-120},"id":"x0Zj-HgOdYDs","outputId":"8ef5e862-0fe9-4606-cc17-6013b23f645b"},"outputs":[],"source":["# Training (fit Neural Network)\n","if(best_network_info['embedding'] == 'Glove'):\n","\n","  training_history = best_network.fit(\n","\n","      x = feature_train_glove,\n","      y = label_train,\n","      batch_size = 512,\n","      epochs = int(best_network_info['best_number_epochs'])\n","\n","  )\n","\n","if(best_network_info['embedding'] == 'Word2Vec'):\n","\n","  training_history = best_network.fit(\n","\n","      x = feature_train_word2vec,\n","      y = label_train,\n","      batch_size = 512,\n","      epochs = int(best_network_info['best_number_epochs'])\n","\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcmOgSJi0CC4"},"outputs":[],"source":["best_network.save('../../results/models/' + network_name +'.h5')"]},{"cell_type":"markdown","metadata":{"id":"VKc5b27OdYDs"},"source":["#### ***Testing***"]},{"cell_type":"markdown","metadata":{"id":"Hoc9ckdguiA6"},"source":["Testing the neural network model with the test set.\n","\n","*   The text in the test set has been vectorized using the Glove embedding created using the training set, keeping the consistency in the results;\n","\n","*   We have evaluated the performance using the **categorical cross entropy loss** amd **global accuracy**;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqNnKgMA4y6O"},"outputs":[],"source":["best_network = load_model('../../results/models/' + network_name +'.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCHhft4UdYDs"},"outputs":[],"source":["if(best_network_info['embedding'] == 'Glove'):\n","\n","  feature_test_glove = text_vectorization.textVectorization(test_set['text'], text_vectorizer_keras)\n","  score = best_network.evaluate(feature_test_glove, label_test, verbose = 0)\n","\n","if(best_network_info['embedding'] == 'Word2Vec'):\n","\n","  feature_test_word2vec = text_vectorization.textVectorizationWord2Vec(test_set['text'], text_vectorizer_word2vec, words_per_sentence)\n","  score = best_network.evaluate(feature_test_word2vec, label_test, verbose = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqckTXJDxbzw"},"outputs":[],"source":["# Performance metrics\n","test_loss = round(score[0], 3)\n","test_accuracy = round(score[1], 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUAvR4O92bya"},"outputs":[],"source":["# Write the testing performance on the global final results CSV\n","with open('../../results/data/results.csv', mode = 'a', newline = '') as file:\n","\n","    writer = csv.writer(file)\n","    writer.writerow([\n","        best_network_info['network'],\n","        best_network_info['embedding'],\n","        best_network_info['data_aug'],\n","        best_network_info['regularization'],\n","        best_network_info['number_of_layers'],\n","        test_loss,\n","        test_accuracy\n","    ])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
