{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "#nltk.data.path.append('C:\\\\Users\\\\della\\\\anaconda3\\\\nltk-data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pre_processing\n",
    "#importlib.reload(pre_processing)\n",
    "\n",
    "import text_vectorization\n",
    "#importlib.reload(text_vectorization)\n",
    "\n",
    "import embedding\n",
    "#importlib.reload(embedding)\n",
    "\n",
    "import kfold_cv\n",
    "importlib.reload(kfold_cv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Reading initial dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv('data/arxiv-dataset-cat1.csv')\n",
    "\n",
    "# Number of different categories\n",
    "number_of_categories = len(arxiv_data['label'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Splitting in training and test set*\n",
    "\n",
    "The initial dataset is partitioned in training and test set, with the following strategy:\n",
    "- **70%** training and **30%** test;\n",
    "- **Stratified sampling** based on the value of the label (categeory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(arxiv_data, test_size = 0.3, stratify = arxiv_data['label'], random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "with open('../glove-embedding.pkl', 'rb') as file:\n",
    "    glove_embedding = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarWords(tokenized_text):\n",
    "\n",
    "    similar_words_total = []\n",
    "\n",
    "    for word in tokenized_text:\n",
    "\n",
    "        try:\n",
    "\n",
    "            word_embedding = glove_embedding[word]\n",
    "            distances = np.dot(list(glove_embedding.values()), word_embedding)\n",
    "            most_similar_indices = np.argsort(-distances)[:5]\n",
    "            similar_words = [ list(glove_embedding.keys())[i] for i in most_similar_indices ]\n",
    "\n",
    "            similar_words_total = similar_words_total + similar_words\n",
    "        \n",
    "        except KeyError:\n",
    "\n",
    "            similar_words_total = similar_words_total + []\n",
    "\n",
    "    return similar_words_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomInsert(text, n):\n",
    "\n",
    "    index = 0\n",
    "    tokenized_text = random.sample(nltk.word_tokenize(text), k = 5)\n",
    "    augmented_tokenized_text = tokenized_text.copy()\n",
    "\n",
    "    similar_words = random.sample(getSimilarWords(tokenized_text), k = n)\n",
    "\n",
    "    while(index < n):\n",
    "\n",
    "        new_word_index = random.randint(0, len(augmented_tokenized_text) - 1)\n",
    "        augmented_tokenized_text.insert(new_word_index, similar_words[index])\n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    return \" \".join(augmented_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The this cat in an mats of an be is that that has sitting i on dog cats sat the on mat\n"
     ]
    }
   ],
   "source": [
    "original_text = \"The cat is sitting on the mat\"\n",
    "\n",
    "augmented_texts = randomInsert(original_text, n = 15)\n",
    "\n",
    "print(augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_augmented"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class imbalance\n",
    "- Number of words per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF WORDS IMBALANCE\n",
    "x = train_set['text'].apply(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS IMBALANCE\n",
    "round(train_set['label'].value_counts() / sum(train_set['label'].value_counts()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['words'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_set.groupby('label')['words'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['words'] / y['words'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[train_set['words'] < 20]['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = arxiv_data.iloc[x[x > 20].index, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Pre-processing on training set*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A phase of pre-processing is applied to the textual observation of the training set, with the following operations:\n",
    "- Converting in **lower case**;\n",
    "- Removing **special characters and symbols**;\n",
    "- Removing **stop words**;\n",
    "- **Lemmatization**.\n",
    "\n",
    "Therefore, the labels associated with articles' categories are converted in **one-hot representation**.\n",
    "\n",
    "*The processed dataset has been saved in a CSV file, in such a way that it is not necessary to repeat the time-consuming procedure each time the code is executed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set_processed = pre_processing.dataPreProcessing(train_set)\n",
    "#train_set_processed.to_csv('data/train-set-cat1-processed.csv', index = False)\n",
    "\n",
    "train_set = pd.read_csv('data/train-set-cat1-processed.csv')\n",
    "\n",
    "# One-hot encoding of the labels\n",
    "label_train = to_categorical(train_set['label'], num_classes = number_of_categories, dtype = 'int64') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data augmentation*\n",
    "We have decided to use some **data augmentation techniques** with the aim of balancing the training set, considering these issues:\n",
    "- **Number of words per observation**\\\n",
    "About this problem, a **threshold** of at least **15 words per observation** has been setted. In order to obtain this result, the **Random Insertion** technique has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_augmented = train_set.copy()\n",
    "\n",
    "# Count the number of words for each observation\n",
    "train_set_augmented['words'] = train_set_augmented['text'].apply(lambda row: len(row.split()))\n",
    "\n",
    "# Min number of words per observation\n",
    "min_number_words = 15\n",
    "\n",
    "# Set of observations with less than the threshold (needed data augmentation)\n",
    "observations_to_augment = train_set_augmented[train_set_augmented['words'] < min_number_words]\n",
    "\n",
    "#while(len(observations_to_augment) != 0):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(observations_to_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = observations_to_augment['text'].iloc[1:10].apply(lambda row: randomInsert(row, min_number_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8      be kuh n't i commutative sah hypersemigroups a...\n",
       "42     domenico summary giuseppe contemporary mechani...\n",
       "61     fax c = group group lopez f optical holographi...\n",
       "77     einstein pictorial make 1-column pictorial phy...\n",
       "91     mitul semigroups awards homotopy polytopes sem...\n",
       "163    axis algebraic spheres linked suspected abelia...\n",
       "167    research meaning kerner sexual mental cognitiv...\n",
       "275    lengthwise crosswise shake slice you me crossw...\n",
       "347    build shelah built shelah abelian formula_2 co...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in x.index:\n",
    "\n",
    "    observations_to_augment.loc[f,'text'] = x[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\della\\AppData\\Local\\Temp\\ipykernel_20884\\3327876522.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations_to_augment.loc[:,'words'] = observations_to_augment['text'].apply(lambda row: len(row.split()))\n"
     ]
    }
   ],
   "source": [
    "observations_to_augment.loc[:,'words'] = observations_to_augment['text'].apply(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>three analogue stern diatomic sequence sam nor...</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>veritas collaboration contribution st internat...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>figure surgery stimes motoo tange paper withdr...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>algebra infinite qubit system g sardanashvily ...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>smooth homotopy sphere akio kawauchi every smo...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53903</th>\n",
       "      <td>mathematics liquid crystal john ball review gi...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53916</th>\n",
       "      <td>mathematical caricature large wave mikhail kov...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53933</th>\n",
       "      <td>supersymmetry dimensional system enrico deotto...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53962</th>\n",
       "      <td>inversion adjunction log canonicity masayuki k...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54001</th>\n",
       "      <td>humancompetitive award w b langdon report humi...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1662 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  words\n",
       "5      three analogue stern diatomic sequence sam nor...      3     13\n",
       "420    veritas collaboration contribution st internat...      4     14\n",
       "453    figure surgery stimes motoo tange paper withdr...      3      9\n",
       "473    algebra infinite qubit system g sardanashvily ...      3     14\n",
       "508    smooth homotopy sphere akio kawauchi every smo...      3     11\n",
       "...                                                  ...    ...    ...\n",
       "53903  mathematics liquid crystal john ball review gi...      4     14\n",
       "53916  mathematical caricature large wave mikhail kov...      4     14\n",
       "53933  supersymmetry dimensional system enrico deotto...      4     11\n",
       "53962  inversion adjunction log canonicity masayuki k...      3     11\n",
       "54001  humancompetitive award w b langdon report humi...      0     10\n",
       "\n",
       "[1662 rows x 3 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_to_augment[observations_to_augment['words'] < min_number_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     be kuh n't i commutative sah hypersemigroups a...\n",
       "label                                                    3\n",
       "words                                                   20\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_to_augment.loc[8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_to_augment.loc[8, 'text'] = x[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our feudal semigroups insectivores semigroups hypersemigroups niovi serf i kehayopulu matroid paper je are lattices newspapers movie serf example show way pas it you that hah semigroups gammasemigroups hypersemigroups'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_to_augment.loc[8, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\della\\AppData\\Local\\Temp\\ipykernel_20884\\4084204227.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations_to_augment.loc[8, 0] = x\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible indexer with Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m observations_to_augment\u001b[39m.\u001b[39;49mloc[\u001b[39m8\u001b[39;49m, \u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\della\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\della\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj[key] \u001b[39m=\u001b[39m infer_fill_value(value)\n\u001b[0;32m   1644\u001b[0m     new_indexer \u001b[39m=\u001b[39m convert_from_missing_indexer_tuple(\n\u001b[0;32m   1645\u001b[0m         indexer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39maxes\n\u001b[0;32m   1646\u001b[0m     )\n\u001b[1;32m-> 1647\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer(new_indexer, value, name)\n\u001b[0;32m   1649\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m \u001b[39m# reindex the axis\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m \u001b[39m# make sure to clear the cache because we are\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[39m# just replacing the block manager here\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[39m# so the object is the same\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\della\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1688\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1687\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1688\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1689\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\della\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1709\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1706\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, ABCSeries) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[0;32m   1707\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m Series\n\u001b[1;32m-> 1709\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_align_series(indexer, Series(value))\n\u001b[0;32m   1711\u001b[0m \u001b[39m# Ensure we have something we can iterate over\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m info_axis \u001b[39m=\u001b[39m indexer[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\della\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2148\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[1;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   2146\u001b[0m     \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39mreindex(ax)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 2148\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIncompatible indexer with Series\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible indexer with Series"
     ]
    }
   ],
   "source": [
    "observations_to_augment.loc[8] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    use semigroups semigroups hypersemigroups niov...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'semigroups semigroups hypersemigroups niovi kehayopulu paper serf example show way pas semigroups gammasemigroups hypersemigroups'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_to_augment['text'].iloc[1:2][8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Text vectorization and embedding*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstable the following **parameters** are defined:\n",
    "- **Size of the vocabulary** to create;\n",
    "- **Number of words** considered for each text (article);\n",
    "- **Dimension of the embedding**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "words_per_sentence = 200\n",
    "embedding_dim = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have opted for **two different approaches**:\n",
    "\n",
    "- **Version 1: Keras vectorization and GloVe embedding**\\\n",
    "In this scenario, the vectorization (and so the creation of the vocabulary) is carried out using the **Keras built-in function**, with the final adaption of the text vectorizer on the training set. For the embedding matrix, we have used a pre-trained solution, named **GloVe**, with 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer_keras = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n",
    "vocabulary_keras = text_vectorizer_keras.get_vocabulary()\n",
    "\n",
    "embedding_matrix_glove = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary_keras)\n",
    "embedding_layer_glove = embedding.createEmbeddingLayer(embedding_matrix_glove, None)\n",
    "\n",
    "feature_train_glove = text_vectorization.textVectorization(train_set['text'], text_vectorizer_keras)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Version 2: Word2Vec vectorization and embedding**\\\n",
    "The second strategy plans to use a text vectorizer, a vocabulary and an embedding using a **Word2Vec model** directly trained and created on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = text_vectorization.createTextVectorizerWord2Vec(train_set['text'], vocabulary_size, embedding_dim)\n",
    "text_vectorizer_word2vec = word2vec['text_vectorizer']\n",
    "vocabulary_word2vec = list(word2vec['vocabulary_embedding'].key_to_index)\n",
    "\n",
    "embedding_matrix_word2vec = embedding.buildingEmbeddingMatrixWord2Vec(embedding_dim, vocabulary_word2vec, word2vec['vocabulary_embedding'])\n",
    "embedding_layer_word2vec = embedding.createEmbeddingLayer(embedding_matrix_word2vec, None)\n",
    "\n",
    "feature_train_word2vec = text_vectorization.textVectorizationWord2Vec(train_set['text'], text_vectorizer_word2vec, words_per_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Neural network architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n",
    "\n",
    "# Embedding layer\n",
    "x = embedding_layer_glove(input_layer)\n",
    "\n",
    "# Hidden layers\n",
    "x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(rate = 0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n",
    "output_layer = x\n",
    "\n",
    "# Neural network model\n",
    "network = keras.Model(input_layer, output_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *K-Fold Cross Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'batch_size': 128\n",
    "\n",
    "}, \n",
    "\n",
    "{\n",
    "\n",
    "    'filters': 128,\n",
    "    'kernel_size': 5,\n",
    "    'rate': 0.5,\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': 128\n",
    "\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_results = kfold_cv.kfoldCrossValidation(2, feature_train, label_train, network, hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
