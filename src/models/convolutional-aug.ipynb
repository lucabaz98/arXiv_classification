{"cells":[{"cell_type":"markdown","metadata":{"id":"AyzoBaM-dYDW"},"source":["### ***Convolutional Neural Network - Data Augmentation***"]},{"cell_type":"markdown","metadata":{"id":"fa9S0xVm4HWS"},"source":["#### ***Initial operations***"]},{"cell_type":"markdown","metadata":{"id":"ARX9Y6X5_Jtt"},"source":["Firstly, we have done some initial and setting operations, like connecting to our Google Drive folder and importing libraries and files useful for the project."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuxbTGxHdqfV"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import importlib\n","import itertools\n","import csv\n","import sys\n","import os\n","\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from keras.regularizers import l2\n","\n","parent_folder = os.path.abspath('../../')\n","sys.path.append(parent_folder)\n","\n","from src.utils import text_vectorization\n","# importlib.reload(text_vectorization)\n","\n","from src.utils import embedding\n","# importlib.reload(embedding)\n","\n","from src.utils import kfold_cv\n","# importlib.reload(kfold_cv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZruVorigK9e"},"outputs":[],"source":["network_name = 'conv-aug'\n","model_name = 'conv'"]},{"cell_type":"markdown","metadata":{"id":"IKq2O2PGdYDe"},"source":["#### ***Training and test set***"]},{"cell_type":"markdown","metadata":{"id":"uI1QZHeGdYDf"},"source":["In this first stage, we have:\n","- Read the **training and test set**;\n","- Calculated the **number of unique categories**, so the number of classes in the text classification;\n","- Converted the labels associated with the articles' to **one-hot encoding representation**, which is a deep learning best practice when we cope with multi-label text classification task."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N67R0w5Sv7ck"},"outputs":[],"source":["train_set = pd.read_csv('../../data/augmented/train-set-cat1-augmented.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YzTL-jJdYDf"},"outputs":[],"source":["test_set = pd.read_csv('../../data/processed/test-set-cat1-processed.csv')\n","\n","# Number of different categories\n","number_of_categories = len(train_set['label'].unique())\n","\n","# One-hot encoding of the labels\n","label_train = to_categorical(train_set['label'], num_classes = number_of_categories, dtype = 'int64')\n","label_test = to_categorical(test_set['label'], num_classes = number_of_categories, dtype = 'int64')"]},{"cell_type":"markdown","metadata":{"id":"LyGUN5BYdYDg"},"source":["#### ***Text vectorization and embedding***"]},{"cell_type":"markdown","metadata":{"id":"NubDDxHFdYDi"},"source":["Firstly, the following **parameters** are defined:\n","- **Size of the vocabulary** to create;\n","- **Number of words** considered for each text (article);\n","- **Dimension of the embedding**;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-mnTyoldYDj"},"outputs":[],"source":["vocabulary_size = 50000\n","words_per_sentence = 200\n","embedding_dim = 100"]},{"cell_type":"markdown","metadata":{"id":"9ivQsA3mdYDj"},"source":["Then, we have carried out two embedding approaches:\n","\n","- **Keras vectorization and GloVe embedding**\n","\n","  - The *vectorization* (and so the creation of the *vocabulary*) is carried out using the **Keras built-in function**, with the final adaption of the text vectorizer on the training set;\n","  - For the *embedding matrix*, we have used a pre-trained solution, named **GloVe**, with 100 dimensions;\n","  - Finally, we have created the final **vectorized feature** for the training phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTgwgvDudYDk"},"outputs":[],"source":["text_vectorizer_keras = text_vectorization.createTextVectorizer(vocabulary_size, words_per_sentence, train_set['text'])\n","vocabulary_keras = text_vectorizer_keras.get_vocabulary()\n","\n","embedding_matrix_glove = embedding.buildEmbeddingMatrix(embedding_dim, vocabulary_keras)\n","embedding_layer_glove = embedding.createEmbeddingLayer(embedding_matrix_glove, None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6cMmMiLw_rT"},"outputs":[],"source":["feature_train_glove = text_vectorization.textVectorization(train_set['text'], text_vectorizer_keras)"]},{"cell_type":"markdown","metadata":{"id":"ttng3QL_wxxi"},"source":["- **Word2Vec vectorization and embedding**\n","\n","  - This strategy plans to create a text vectorizer, a vocabulary and an embedding using a *Word2Vec model* directly trained on our training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioMHcdj3wyJR"},"outputs":[],"source":["word2vec = text_vectorization.createTextVectorizerWord2Vec(train_set['text'], vocabulary_size, embedding_dim)\n","text_vectorizer_word2vec = word2vec['text_vectorizer']\n","vocabulary_word2vec = list(word2vec['vocabulary_embedding'].key_to_index)\n","\n","embedding_matrix_word2vec = embedding.buildingEmbeddingMatrixWord2Vec(embedding_dim, vocabulary_word2vec, word2vec['vocabulary_embedding'])\n","embedding_layer_word2vec = embedding.createEmbeddingLayer(embedding_matrix_word2vec, None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cx4I5utMxAlF"},"outputs":[],"source":["feature_train_word2vec = text_vectorization.textVectorizationWord2Vec(train_set['text'], text_vectorizer_word2vec, words_per_sentence)"]},{"cell_type":"markdown","metadata":{"id":"X1aJfLrqdYDk"},"source":["#### ***Neural network architectures***"]},{"cell_type":"markdown","metadata":{"id":"mnHYHXk7hLhT"},"source":["Here, we have defined a **set of Convolutional Neural Network architectures** (models), using different combinations of hyperparameters:\n","\n","- *Embedding layer*: Glove or Word2Vec (as explained before);\n","- *Number of convolutional hidden layers*: 1, 2 or 3;\n","- *Number of filters in the convolutional layers*: 128 or 256;\n","- *Kernel size of the convolutional layers*: 3, 4 or 5;\n","- *Regularization*: L2 regularization in the convolutional layers;\n","\n","The following parameters have kept the same value in each architectures:\n","- *Learning rate*: 0.5;\n","- *Batch size*: 512."]},{"cell_type":"markdown","metadata":{"id":"xgGCT7qu9uRo"},"source":["There will be a **list of neural network architecture**, which also a brief explanation:"]},{"cell_type":"markdown","metadata":{"id":"ekjg8yF496YU"},"source":["##### **Neural network A - 3 Layers, Same Number of Layers, Decrease Kernel Size**\n","  - *Glove embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 5;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 4;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QzcvG6y95i3"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 4, activation = 'relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation = 'relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_A = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_A_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 4,\n","\n","    'layer3_filters': 128,\n","    'layer3_kernel_size': 3\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"B8wLxDdhAxu-"},"source":["##### **Neural network B - 3 Layers, Same Number of Layers, Same Kernel Size**\n","  - *Glove embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FebPFxC7xa-3"},"outputs":[],"source":["input_layer = keras.Input(shape = (words_per_sentence,), dtype = 'int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_B = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_B_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': 128,\n","    'layer3_kernel_size': 3\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"SNU_QwjTEKTn"},"source":["##### **Neural network C - 3 Layers, Glove versus Word2Vec**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxZ4ykOSELN7"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_C = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_C_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': 128,\n","    'layer3_kernel_size': 3\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"l0KGzs4CEytl"},"source":["##### **Neural network D - 3 Layers, Increase Number of Layers**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 32 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wau28AsrEk9W"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 32, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_D = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_D_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_filters': 32,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': 64,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': 128,\n","    'layer3_kernel_size': 3\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"yQB6F8W_SoQh"},"source":["##### **Neural network E - 3 Layers, Increase but Same Kernel Size**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 32 and kernel size 5;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 5;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaIp6lBZTMYS"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 32, kernel_size = 5, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_E = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_E_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_filters': 32,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': 64,\n","    'layer2_kernel_size': 5,\n","\n","    'layer3_filters': 128,\n","    'layer3_kernel_size': 5\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"8QlnSbUxTpF3"},"source":["##### **Neural network F - 3 Layers, With Regularization**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 32 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *With regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6O5JTMtqTpwG"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 32, kernel_size = 3, activation='relu', kernel_regularizer = l2(0.01))(x)\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 3, activation='relu', kernel_regularizer = l2(0.01))(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu', kernel_regularizer = l2(0.01))(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_F = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_F_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': True,\n","\n","    'number_of_layers': 3,\n","\n","    'layer1_filters': 32,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': 64,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': 128,\n","    'layer3_kernel_size': 3\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"bemELSbvFSV_"},"source":["##### **Neural network G - 2 Layers, Same Number of Layers, Decrease Kernel Size**\n","  - *Glove embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 4;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lyalrf0EFStn"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 4, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_G = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_G_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 4,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"_MH7DX9iMG1z"},"source":["##### **Neural network H - 2 Layers, Same Number of Layers, Same Kernel Size**\n","  - *Glove embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TI4lZxJMHUx"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_H = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_H_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': 64,\n","    'layer2_kernel_size': 5,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"BfwiDw7EN-mw"},"source":["##### **Neural network I - 2 Layers, Glove versus Word2Vec**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6bi3rhRN-98"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_I = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_I_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': 64,\n","    'layer2_kernel_size': 5,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"qz7Tg09wPcGD"},"source":["##### **Neural network L - 2 Layers, Increase Number of Layers**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftbcqiZhPcqV"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 3, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_L = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_L_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"euTDmNnHXD16"},"source":["##### **Neural network M - 2 Layers, Increase but Same Kernel Size**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 5;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmWaKKlEXQ9W"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu')(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_M = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_M_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 5,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"qyVaDtUNXhJM"},"source":["##### **Neural network N - 2 Layers, Increase Number of Layers**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 3;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *With regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdXEZk3WXnHq"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 3, activation='relu', kernel_regularizer = l2(0.01))(x)\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu', kernel_regularizer = l2(0.01))(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_N = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_N_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': True,\n","\n","    'number_of_layers': 2,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': 128,\n","    'layer2_kernel_size': 3,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"gWiwhPdNR1P1"},"source":["##### **Neural network O - 1 Layer, Glove**\n","  - *Glove embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kRbgDE7R1ix"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_glove(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_O = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_O_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Glove',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': None,\n","    'layer2_kernel_size': None,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"qbNlPkugR16J"},"source":["##### **Neural network P - 1 Layer, Word2Vec**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAUjQNaZR2Mj"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_P = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_P_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': None,\n","    'layer2_kernel_size': None,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"gRVlgY39R2kp"},"source":["##### **Neural network Q - 1 Layer, Increase Kernel Size**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 128 and kernel size 5;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Twq691NYR23_"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 128, kernel_size = 5, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_Q = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_Q_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_filters': 128,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': None,\n","    'layer2_kernel_size': None,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"-xXKkEfaR3ST"},"source":["##### **Neural network R - 1 Layer, Decrease Number of Filters**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 3;\n","  - *Without regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6ZDHZ8XR3vt"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 3, activation='relu')(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_R = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_R_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': False,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 3,\n","\n","    'layer2_filters': None,\n","    'layer2_kernel_size': None,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"J1GHJkZ4ZE8E"},"source":["##### **Neural network S - 1 Layer, With Regularization**\n","  - *Word2Vec embedding*;\n","  - *1 convolutional layer*: number of filters equal to 64 and kernel size 5;\n","  - *With regularization*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDW_bL8ZZFhN"},"outputs":[],"source":["input_layer = keras.Input(shape=(words_per_sentence,), dtype='int64')\n","\n","x = embedding_layer_word2vec(input_layer)\n","\n","x = keras.layers.Conv1D(filters = 64, kernel_size = 5, activation='relu', kernel_regularizer = l2(0.01))(x)\n","\n","x = keras.layers.GlobalMaxPooling1D()(x)\n","\n","x = keras.layers.Dropout(0.5)(x)\n","\n","x = keras.layers.Dense(number_of_categories, activation = 'softmax')(x)\n","output_layer = x\n","\n","conv_network_S = keras.Model(input_layer, output_layer, name = model_name)\n","\n","conv_network_S_info = {\n","\n","    'network': model_name,\n","    'data_aug': True,\n","\n","    'embedding': 'Word2Vec',\n","    'regularization': True,\n","\n","    'number_of_layers': 1,\n","\n","    'layer1_filters': 64,\n","    'layer1_kernel_size': 5,\n","\n","    'layer2_filters': None,\n","    'layer2_kernel_size': None,\n","\n","    'layer3_filters': None,\n","    'layer3_kernel_size': None\n","\n","}\n","\n","del input_layer, x, output_layer"]},{"cell_type":"markdown","metadata":{"id":"s62OXiC5mRdV"},"source":["##### **Neural networks sets and other information**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Zd8MiSHdYDp"},"outputs":[],"source":["network_models_set = [\n","    conv_network_A, conv_network_B, conv_network_C, conv_network_D,\n","    conv_network_E, conv_network_F, conv_network_G, conv_network_H,\n","    conv_network_I, conv_network_L, conv_network_M, conv_network_N,\n","    conv_network_O, conv_network_P, conv_network_Q, conv_network_R,\n","    conv_network_S\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecfgbYuPmQ00"},"outputs":[],"source":["initial_weights = [\n","    conv_network_A.get_weights(), conv_network_B.get_weights(), conv_network_C.get_weights(), conv_network_D.get_weights(),\n","    conv_network_E.get_weights(), conv_network_F.get_weights(), conv_network_G.get_weights(), conv_network_H.get_weights(),\n","    conv_network_I.get_weights(), conv_network_L.get_weights(), conv_network_M.get_weights(), conv_network_N.get_weights(),\n","    conv_network_O.get_weights(), conv_network_P.get_weights(), conv_network_Q.get_weights(), conv_network_R.get_weights(),\n","    conv_network_S.get_weights()\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_SAbxsxZDLH"},"outputs":[],"source":["network_info_set = [\n","    conv_network_A_info, conv_network_B_info, conv_network_C_info, conv_network_D_info,\n","    conv_network_E_info, conv_network_F_info, conv_network_G_info, conv_network_H_info,\n","    conv_network_I_info, conv_network_L_info, conv_network_M_info, conv_network_N_info,\n","    conv_network_O_info, conv_network_P_info, conv_network_Q_info, conv_network_R_info,\n","    conv_network_S_info\n","]"]},{"cell_type":"markdown","metadata":{"id":"ZRUDu3bzdYDn"},"source":["#### ***K-Fold Cross Validation***"]},{"cell_type":"markdown","metadata":{"id":"LZXGj84uhufb"},"source":["In this part of the project, we have implemented the **K-Fold Cross Validation** as a strategy to find the **best hyperparameters** for the neural network and also to have a **performance estimation** of the model on new and unseen data. Our approach has followed these logic:\n","\n","*   Firstly, we defined a **number of epochs** equal to *30*, which will be an upper bound in the actual number of epochs used to train the model, due to the fact that we have used an *early stopping monitoring rule*: if the performance does not improve for 3 straight epochs, the K-Fold cycle end and we keep the epoch number with the best performance as hypeparameter;\n","\n","*   The **number of folds K** has been set to *3* and a multi-label stratified approach has been carried out;\n","\n","*   As a text classification task (categorical label), the **loss function** has been the **categorical cross entropy**, which will result in a loss value. Our goal is to **minimize** this metric, in order to improve the performance of the model, so we have used it as our performance proxy. Also, we have taken into account the **accuracy**;\n","\n","*   To evaluate a single moodel (combination of hyperparameters), we have computed the **average of the performance** of the K iteration;\n","\n","*   *The best network architecture is the one which lead to the lowest loss*;\n","\n","*   *We write a CSV file with all the different networks architecture and the related obtained performance in the K-Fold CV*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoMaZxYegS6G"},"outputs":[],"source":["epochs = 30\n","k_fold = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYt207UhdYDq"},"outputs":[],"source":["kfold_results = []\n","\n","# For each neural network architecture\n","for index, element in enumerate(network_models_set):\n","\n","    # Print information to manage the situation during the process\n","    print(f\"Neural network {index}\")\n","\n","    # Performing the K-Fold Cross Validation\n","    if(network_info_set[index]['embedding'] == 'Glove'):\n","\n","      kfold_result = kfold_cv.kfoldCrossValidation(k_fold, feature_train_glove, label_train, element, network_info_set[index], epochs)\n","\n","    if(network_info_set[index]['embedding'] == 'Word2Vec'):\n","\n","      kfold_result = kfold_cv.kfoldCrossValidation(k_fold, feature_train_word2vec, label_train, element, network_info_set[index], epochs)\n","\n","    kfold_results.append(kfold_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHeeRZVxdYDr"},"outputs":[],"source":["# Write the K-Fold CV results to a CSV file\n","with open('../../results/data/kfold-' + network_name + '.csv', mode = 'w', newline = '') as file:\n","\n","    writer = csv.DictWriter(file, fieldnames = list(kfold_results[0].keys()))\n","    writer.writeheader()\n","\n","    for row_data in kfold_results:\n","        writer.writerow(row_data)"]},{"cell_type":"markdown","metadata":{"id":"eL-oU7khscLL"},"source":["#### ***Convolutional Neural Network - Final Architecture***"]},{"cell_type":"markdown","metadata":{"id":"SVkGQIL1swA7"},"source":["Here, we have created the **neural network architecture** model with the best hyperparameters found in the K-Fold Cross Validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laknVnOQdYDr"},"outputs":[],"source":["# Get the best hyperparameters combination\n","best_loss = 999\n","best_network = None\n","best_network_info = None\n","\n","for index, element in enumerate(kfold_results):\n","\n","    if(element['loss'] < best_loss):\n","\n","        best_loss = element['loss']\n","\n","        best_network = network_models_set[index]\n","        best_network.set_weights(initial_weights[index])\n","\n","        best_network_info = network_info_set[index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73Zvs_DIdYDr"},"outputs":[],"source":["# Compiling the network\n","best_network.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"09aHqRE1dYDs"},"source":["#### ***Training***"]},{"cell_type":"markdown","metadata":{"id":"A1-LV85MuAvo"},"source":["Training the neural network model with all the training data and save the H5 model file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0Zj-HgOdYDs"},"outputs":[],"source":["# Training (fit Neural Network)\n","if(best_network_info['embedding'] == 'Glove'):\n","\n","  training_history = best_network.fit(\n","\n","      x = feature_train_glove,\n","      y = label_train,\n","      batch_size = 512,\n","      epochs = int(best_network_info['best_number_epochs'])\n","\n","  )\n","\n","if(best_network_info['embedding'] == 'Word2Vec'):\n","\n","  training_history = best_network.fit(\n","\n","      x = feature_train_word2vec,\n","      y = label_train,\n","      batch_size = 512,\n","      epochs = int(best_network_info['best_number_epochs'])\n","\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcmOgSJi0CC4"},"outputs":[],"source":["best_network.save('../../results/models/' + network_name +'.h5')"]},{"cell_type":"markdown","metadata":{"id":"VKc5b27OdYDs"},"source":["#### ***Testing***"]},{"cell_type":"markdown","metadata":{"id":"Hoc9ckdguiA6"},"source":["Testing the neural network model with the test set.\n","\n","*   The text in the test set has been vectorized using the Glove embedding created using the training set, keeping the consistency in the results;\n","\n","*   We have evaluated the performance using the **categorical cross entropy loss**, **global accuracy** and **single class accuracy**;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqNnKgMA4y6O"},"outputs":[],"source":["best_network = load_model('../../results/models/' + network_name +'.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCHhft4UdYDs"},"outputs":[],"source":["if(best_network_info['embedding'] == 'Glove'):\n","\n","  feature_test_glove = text_vectorization.textVectorization(test_set['text'], text_vectorizer_keras)\n","  score = best_network.evaluate(feature_test_glove, label_test, verbose = 0)\n","\n","if(best_network_info['embedding'] == 'Word2Vec'):\n","\n","  feature_test_word2vec = text_vectorization.textVectorizationWord2Vec(test_set['text'], text_vectorizer_word2vec, words_per_sentence)\n","  score = best_network.evaluate(feature_test_word2vec, label_test, verbose = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqckTXJDxbzw"},"outputs":[],"source":["# Performance metrics\n","test_loss = round(score[0], 3)\n","test_accuracy = round(score[1], 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUAvR4O92bya"},"outputs":[],"source":["# Write the testing performance on the global final results CSV\n","with open('../../results/data/results.csv', mode = 'a', newline = '') as file:\n","\n","    writer = csv.writer(file)\n","    writer.writerow([\n","        best_network_info['network'],\n","        best_network_info['embedding'],\n","        best_network_info['data_aug'],\n","        best_network_info['regularization'],\n","        best_network_info['number_of_layers'],\n","        test_loss,\n","        test_accuracy\n","    ])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
