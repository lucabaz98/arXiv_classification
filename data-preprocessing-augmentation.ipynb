{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pre_processing\n",
    "#importlib.reload(pre_processing)\n",
    "\n",
    "import data_augmentation\n",
    "#importlib.reload(data_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Reading initial dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ArXiv dataset with the data which will be used during the whole deep learning project has been read from a CSV file.\\\n",
    "The initial dataset does have **77208 rows** (articles), with the following structure in terms of columns:\n",
    "- **text**, a string with the article text that will be analyzed and manipulating in our text classification task;\n",
    "- **label**, the category (level 1) related to the text (for example, physics), which will be the target variable of our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv('data/arxiv-dataset-cat1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Splitting in training and test set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataset is partitioned in training and test set, with the following strategy:\n",
    "- **70%** training and **30%** test;\n",
    "- **Stratified sampling** based on the value of the label (categeory).\n",
    "\n",
    "*Training and test sets have been stored in two CSV files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(arxiv_data, test_size = 0.3, stratify = arxiv_data['label'], random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.to_csv('data/train-set-cat1.csv', index = False)\n",
    "# test_set.to_csv('data/test-set-cat1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Pre-processing on training and test set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A phase of pre-processing is applied to the textual observation of the training and also test set, with the following operations:\n",
    "- Converting in **lower case**;\n",
    "- Removing **special characters and symbols**;\n",
    "- Removing **stop words**;\n",
    "- **Lemmatization**.\n",
    "\n",
    "*The processed datasets has been saved in CSV files, in such a way that it is not necessary to repeat the time-consuming procedure each time the code is executed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_processed = pre_processing.dataPreProcessing(train_set)\n",
    "# train_set_processed.to_csv('data/train-set-cat1-processed.csv', index = False)\n",
    "\n",
    "train_set = pd.read_csv('data/train-set-cat1-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set_processed = pre_processing.dataPreProcessing(test_set)\n",
    "#test_set_processed.to_csv('data/test-set-cat1-processed.csv', index = False)\n",
    "\n",
    "test_set = pd.read_csv('data/test-set-cat1-processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data augmentation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use some **data augmentation techniques** with the aim of balancing the training set, considering these issues:\n",
    "- **Number of words per observation**\n",
    "\n",
    "    About this problem, a **threshold** of at least **15 words per observation** has been set. In order to obtain this result, the **Random Insertion** technique has been used.\\\n",
    "    Our developed **algorithm** follows these steps:\n",
    "    \n",
    "    1. For each observation which does have less words than the threshold, get a **random sample of 5 words** from the initial text;\n",
    "    2. Then, get the **5 most similar words** for each of the previously retrieved tokens and from this set, get **n random words** (we set *n* equal to *15*);\n",
    "    3. Insert the **n words** in random positions within the starting text and substitute the augmented text in the train set;\n",
    "    4. Redo the **pre-processing phase**, due to the fact that the random insertion could have include words we do not accept in our context (like stop words). In this way, we also apply the formatting;\n",
    "    5. **Filter** another time the train set with the aim of finding \"under-threshold\" observations (pre-processing could have deleted some words);\n",
    "    6. Repeat the algorithm until there are no more observations to augment.\n",
    "\n",
    "*This procedure is large time and computational consuming, so the resulted dataframe has been saved in a CSV file and then it will be read to pursue the project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_augmented = train_set.copy()\n",
    "\n",
    "# Count the number of words for each observation\n",
    "train_set_augmented['words'] = train_set_augmented['text'].apply(lambda row: len(row.split()))\n",
    "\n",
    "# Min number of words per observation\n",
    "min_number_words = 15\n",
    "\n",
    "# Set of observations with less than the threshold (needed data augmentation)\n",
    "observations_to_augment = train_set_augmented[train_set_augmented['words'] < min_number_words]\n",
    "\n",
    "# Only if there are still observations to augment\n",
    "while(len(observations_to_augment) != 0):\n",
    "\n",
    "    # Creating the augmented observations\n",
    "    obs_augmented = observations_to_augment['text'].apply(lambda row: data_augmentation.randomInsert(row, min_number_words))\n",
    "\n",
    "    # For each augmented observation replace the text in the training set and redo the pre-processing\n",
    "    for i in obs_augmented.index:\n",
    "\n",
    "        train_set_augmented.loc[i,'text'] = obs_augmented[i]\n",
    "        train_set_augmented.loc[i,'text'] = pre_processing.dataPreProcessing(pd.DataFrame(train_set_augmented.loc[i,:]).transpose()).loc[i,'text']\n",
    "        train_set_augmented.loc[i,'words'] = len(train_set_augmented.loc[i,'text'].split())\n",
    "\n",
    "    observations_to_augment = train_set_augmented[train_set_augmented['words'] < min_number_words]\n",
    "\n",
    "# Drop the unuseful column and write the CSV (previous operations are computational expensive)\n",
    "train_set_augmented.drop('words', axis = 1, inplace = True)\n",
    "\n",
    "# train_set_augmented.to_csv('data/train-set-cat1-augmented-words-obs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set after the first phase of the data augmentation\n",
    "train_set_augmented = pd.read_csv('data/train-set-cat1-augmented-words-obs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Class imbalance management**\n",
    "\n",
    "    We noticed the problem of **class imbalance** related to the articles' label, so we decided to apply a strategy based on **Synonym Replacement** technique with the aim of balancing the dataset and also increase the variability before the actual training phase.\\\n",
    "    Our **approach** follows these steps:\n",
    "    1. Select only the classes which need an increase in the number of observation, in detail we have opted for a threshold of less than *5000 observation per class*;\n",
    "    2. For each one of the previous identified classes, a number of new (augmented) texts **n** to create has been defined as the **number of the initial observations of the class divided by two**: this choice is based on the fact that we wanted to keep the *number of \"original\" observations higher than the number of augmented ones*. In this case, the resulting proportion from the process will be *1/3* augmented data and *2/3* original data for the \"challenged\" classes;\n",
    "    3. A sample of **n** elements of the class has been retrieved from the intial dataset, as the starting point for the data augmentation techinque;\n",
    "    4. For each one of these observations, an augmented text has been created using the **Synonym Replacement algorithm**, which substitutes each word of the text with one synonym (we used the NLTK library);\n",
    "    5. The new data has been concatenated to the train set, resulting in an increase of observations for the considered classes and so an improvement in the balance of the dataset;\n",
    "\n",
    "*This procedure is large time and computational consuming, so the resulted dataframe has been saved in a CSV file and then it will be read to pursue the project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of observations for each class before data augmentation (class imbalance detection)\n",
    "round(train_set_augmented['label'].value_counts() / sum(train_set['label'].value_counts()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_aug_threshold = 5000\n",
    "\n",
    "obs_per_class = train_set_augmented['label'].value_counts()\n",
    "class_to_balance = np.array(obs_per_class[obs_per_class < class_aug_threshold].index)\n",
    "\n",
    "# For each class to augment\n",
    "for class_label in class_to_balance:\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Dataframe with only the observations of the considered class\n",
    "    class_obs = train_set_augmented[train_set_augmented['label'] == class_label].copy()\n",
    "    num_aug_obs = round(len(class_obs) / 2)\n",
    "\n",
    "    # Get the sample of observations\n",
    "    random_class_obs = class_obs.sample(num_aug_obs)\n",
    "\n",
    "    # Until n augmented observations are created\n",
    "    while(i < num_aug_obs):\n",
    "\n",
    "        aug_obs = data_augmentation.synonymReplacement(random_class_obs.iloc[i,0], 1)[0]\n",
    "        train_set_augmented = pd.concat([ train_set_augmented, pd.DataFrame({ 'text': [aug_obs], 'label': [class_label] }) ])\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "# train_set_augmented.to_csv('data/train-set-cat1-augmented.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set after the data augmentation\n",
    "train_set_augmented = pd.read_csv('data/train-set-cat1-augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of observations for each class after data augmentation\n",
    "round(train_set_augmented['label'].value_counts() / sum(train_set['label'].value_counts()), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
